{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import copy\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['id','text','sentiment']\n",
    "train = pd.read_csv(\"agr_en_train.csv\",header=None, names=cols)\n",
    "train.drop(['id'],axis=1,inplace=True)\n",
    "train['sentiment'] = train['sentiment'].map({'OAG': 0, 'CAG': 1,'NAG': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['id','text','sentiment']\n",
    "dev = pd.read_csv(\"agr_en_dev.csv\",header=None, names=cols)\n",
    "dev.drop(['id'],axis=1,inplace=True)\n",
    "dev['sentiment'] = dev['sentiment'].map({'OAG': 0, 'CAG': 1,'NAG': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = train.append(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "import multiprocessing\n",
    "from sklearn import utils\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "import nltk\n",
    "\n",
    "puncttok = nltk.WordPunctTokenizer().tokenize\n",
    "\n",
    "sp = SpellCorrector(corpus=\"english\") \n",
    "\n",
    "\n",
    "seg_tw = Segmenter(corpus=\"twitter\")\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = re.MULTILINE | re.DOTALL\n",
    "\n",
    "def tokenize(text):\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "    \n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \" url \")\n",
    "    #text = re_sub(r\"#(\\S+)\", r\"\\1\") # replace #name with name\n",
    "    text = re_sub(r\"(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))\", \" em_positive \") # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    text = re_sub(r\"(:\\s?D|:-D|x-?D|X-?D)\", \" em_positive \") # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    text = re_sub(r\"(<3|:\\*)\", \" em_positive \") # Love -- <3, :*\n",
    "    text = re_sub(r\"(;-?\\)|;-?D|\\(-?;)\", \" em_positive \") # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    text = re_sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', \" em_negative \") # Sad -- :-(, : (, :(, ):, )-:\n",
    "    text = re_sub(r'(:,\\(|:\\'\\(|:\"\\()', \" em_negative \") # Cry -- :,(, :'(, :\"(\n",
    "    text = re_sub(r\"(.)\\1+\", r\"\\1\\1\") # remove funnnnny --> funny\n",
    "    text = re_sub(r\"(-|\\')\", \"\") # remove &\n",
    "    #text = re_sub(r\"/\",\" / \")\n",
    "    text = re_sub(r\"@[0-9]+-\", \" number \")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \" em_positive \")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \" em_positive \")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \" em_negative \")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \" em_neutralface \")\n",
    "    #text = re_sub(r\"<3\",\" heart \")\n",
    "    #text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \" \")\n",
    "    #text = re_sub(r\"#\\S+\", hashtag)\n",
    "    #text = re_sub(r\"([!?.]){2,}\", r\" \\1 \")\n",
    "    #text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 \")\n",
    "    #text = re_sub(r\"([A-Z]){2,}\", allcaps)\n",
    "    #text = re_sub(r'([\\w!.,?();*\\[\\]\":\\”\\“])([!.,?();*\\[\\]\":\\”\\“])', r'\\1 \\2')\n",
    "    #text = re_sub(r'([!.,?();*:\\[\\]\":\\”\\“])([\\w!.,?();*\\[\\]\":\\”\\“])', r'\\1 \\2')\n",
    "    #text = re_sub(r'(.)(<)', r'\\1 \\2')\n",
    "    #text = re_sub(r'(>)(.)', r'\\1 \\2')\n",
    "    #text = re_sub(r'[\\'\\`\\’\\‘]', r'')\n",
    "    #text = re_sub(r'\\\\n', r' ')\n",
    "    text = re_sub(r'-', r' ')\n",
    "    #text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \" url \")\n",
    "    text = re_sub(r\"([pls?s]){2,}\", r\"\\1\")\n",
    "    text = re_sub(r\"([plz?z]){2,}\", r\"\\1\")\n",
    "    text = re_sub(r'\\\\n', r' ')\n",
    "    #text = re_sub(r\"<3\",\"love\")\n",
    "    text = re_sub(r\" sx \",\" sex \")\n",
    "    text = re_sub(r\" u \",\" you \")\n",
    "    text = re_sub(r\" r \",\" are \")\n",
    "    text = re_sub(r\" y \",\" why \")\n",
    "    text = re_sub(r\" Y \",\" WHY \")\n",
    "    text = re_sub(r\"Y \",\" WHY \")\n",
    "    text = re_sub(r\" hv \",\" have \")\n",
    "    text = re_sub(r\" c \",\" see \")\n",
    "    text = re_sub(r\" bcz \",\" because \")\n",
    "    text = re_sub(r\" coz \",\" because \")\n",
    "    text = re_sub(r\" v \",\" we \")\n",
    "    text = re_sub(r\" ppl \",\" people \") \n",
    "    text = re_sub(r\" pepl \",\" people \")\n",
    "    text = re_sub(r\" r b i \",\" rbi \")\n",
    "    text = re_sub(r\" R B I \",\" RBI \")\n",
    "    text = re_sub(r\" R b i \",\" rbi \")\n",
    "    text = re_sub(r\" R \",\" ARE \")\n",
    "    text = re_sub(r\" hav \",\" have \")\n",
    "    text = re_sub(r\"R \",\" ARE \")\n",
    "    text = re_sub(r\" U \",\" you \")\n",
    "    text = re_sub(r\" 👎 \",\" OAG \")\n",
    "    text = re_sub(r\"U \",\" you \")\n",
    "    text = re_sub(r\" pls \",\" please \")\n",
    "    text = re_sub(r\"Pls \",\"Please \")\n",
    "    text = re_sub(r\"plz \",\"please \")\n",
    "    text = re_sub(r\"Plz \",\"Please \")\n",
    "    text = re_sub(r\"PLZ \",\"Please \")\n",
    "    text = re_sub(r\"Pls\",\"Please \")\n",
    "    text = re_sub(r\"plz\",\"please \")\n",
    "    text = re_sub(r\"Plz\",\"Please \")\n",
    "    text = re_sub(r\"PLZ\",\"Please \") \n",
    "    text = re_sub(r\" thankz \",\" thanks \")\n",
    "    text = re_sub(r\" thnx \",\" thanks \")\n",
    "    text = re_sub(r\"fuck\\w+ \",\" fuck \")\n",
    "    text = re_sub(r\"f\\*\\* \",\" fuck \")\n",
    "    text = re_sub(r\"\\*\\*\\*k \",\" fuck \")\n",
    "    text = re_sub(r\"F\\*\\* \",\" fuck \")\n",
    "    text = re_sub(r\"mo\\*\\*\\*\\*\\* \",\" fucker \")\n",
    "    text = re_sub(r\"b\\*\\*\\*\\* \",\" blody \")\n",
    "    text = re_sub(r\" mc \",\" fucker \")\n",
    "    text = re_sub(r\" MC \",\" fucker \")\n",
    "    text = re_sub(r\" wtf \",\" fuck \")\n",
    "    text = re_sub(r\" ch\\*\\*\\*ya \",\" fucker \")\n",
    "    text = re_sub(r\" ch\\*\\*Tya \",\" fucker \")\n",
    "    text = re_sub(r\" ch\\*\\*Tia \",\" fucker \")\n",
    "    text = re_sub(r\" C\\*\\*\\*yas \",\" fucker \")\n",
    "    text = re_sub(r\"l\\*\\*\\*\\* \",\"shit \")\n",
    "    text = re_sub(r\" A\\*\\*\\*\\*\\*\\*S\",\" ASSHOLES\")\n",
    "    text = re_sub(r\" di\\*\\*\\*\\*s\",\" cker\")\n",
    "    text = re_sub(r\" nd \",\" and \")\n",
    "    text = re_sub(r\"Nd \",\"and \")\n",
    "    text = re_sub(r\"([!?!]){2,}\", r\"! \")\n",
    "    text = re_sub(r\"([.?.]){2,}\", r\". \")\n",
    "    text = re_sub(r\"([*?*]){2,}\", r\"* \")\n",
    "    text = re_sub(r\"([,?,]){2,}\", r\", \")\n",
    "    text = re_sub(r\"([!]){2,}\", r\"! \")\n",
    "    text = re_sub(r\"([.]){2,}\", r\". \")\n",
    "    text = re_sub(r\"([*]){2,}\", r\"* \")\n",
    "    text = re_sub(r\"([,]){2,}\", r\", \")\n",
    "    text = re_sub(r\"\\n\\r\", \" \")\n",
    "    text = re_sub(r\"(ind[vs]pak)\", \" india versus pakistan \")\n",
    "    text = re_sub(r\"(pak[vs]ind)\", \" pakistan versus india \")\n",
    "    text = re_sub(r\"(indvsuae)\", \" india versus United Arab Emirates \")\n",
    "    text = re_sub(r\"[sS]hut[Dd]own[jnuJNU]\", \" shut down jnu \")\n",
    "    #text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \" number \")\n",
    "    return text\n",
    "\n",
    "def stem(word):\n",
    "\t\tregexp = r'^(.*?)(ing)?$'\n",
    "\t\tstem, suffix = re.findall(regexp, word)[0]\n",
    "\t\treturn stem\n",
    "\n",
    "my_word_stop = ['the','in','of','is','a','to','an','be','are','for','he','she','we','was','it','as','on']\n",
    "    \n",
    "def tokenize_data(data):\n",
    "    tokenized_data = []\n",
    "    for i in range(data.shape[0]):\n",
    "        filtered_words = ' '.join([stem(word) for word in data[i].split(\" \") if word not in my_word_stop])\n",
    "        filtered_words = ' '.join([word for word in data[i].split(\",\")])\n",
    "        filtered_words = ' '.join([word for word in data[i].split(\"!\")])\n",
    "        filtered_words = ' '.join([word for word in data[i].split(\"#\")])\n",
    "        filtered_words = ' '.join([word for word in data[i].split() if word not in my_word_stop])\n",
    "        filtered_words = ' '.join([stem(word) for word in data[i].split()])\n",
    "        filtered_words = ' '.join([stem(word) for word in data[i].split() if word not in (stopwords.words('my_english_words'))])\n",
    "        #tokens = tokenize(filtered_words)\n",
    "        tokenized_data.append(filtered_words)\n",
    "    return tokenized_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "Xs = []\n",
    "ls = tokenize_data(my_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in ls:\n",
    "    Xs.append(text_processor.pre_process_doc(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "count = 0\n",
    "with open('segmentation_train_dev.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in Xs:        \n",
    "        writer.writerow([row])\n",
    "        count = count +1\n",
    "f.close()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"segmentation_fb_train_dev.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0] = tokenize_data(train[0])\n",
    "train_x = train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = my_df.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Tokenizer\n",
    "tokenizer = Tokenizer(lower=False,filters='')\n",
    "# feed our posts to the Tokenizer\n",
    "tokenizer.fit_on_texts(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#Tokenizers come with a convenient list of words and IDs\n",
    "dictionary = tokenizer.word_index\n",
    "# Let's save this out so we can use it later\n",
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)\n",
    "\n",
    "def convert_text_to_index_array(text):\n",
    "    # one really important thing that `text_to_word_sequence` does\n",
    "    # is make all texts the same length -- in this case, the length\n",
    "    # of the longest text in the set.\n",
    "    temp_wordIndices = []\n",
    "    for word in kpt.text_to_word_sequence(text,filters='',lower=False):\n",
    "        if word in dictionary:\n",
    "            temp_wordIndices.append(dictionary[word])\n",
    "    return temp_wordIndices\n",
    "\n",
    "allWordIndices = []\n",
    "# for each post, change each token to its ID in the Tokenizer's word_index\n",
    "for text in train_x:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)\n",
    "\n",
    "# now we have a list of all posts converted to index arrays.\n",
    "# cast as an array for future usage.\n",
    "allWordIndices = np.asarray(allWordIndices)\n",
    "\n",
    "# create one-hot matrices out of the indexed posts\n",
    "train_x = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "# treat the labels as categories\n",
    "train_y = keras.utils.to_categorical(train_y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation , LSTM , Input , Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Dense, concatenate, Activation, Average\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout , Bidirectional\n",
    "from keras.layers import Flatten , LSTM , Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.regularizers import L1L2\n",
    "from keras import optimizers\n",
    "from keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(150, activation='relu',input_shape=(train_x.shape[1],)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(300, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "filepath=\"sequencing_the_data_try_n_error.{epoch:02d}-{val_loss:.4f}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "csv_logger = CSVLogger('final_log.csv', append=True, separator=';')\n",
    "\n",
    "model.fit(train_x, train_y,\n",
    "    batch_size=50,\n",
    "    epochs=2,\n",
    "    verbose=1,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True,callbacks = [csv_logger])\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model.h5')\n",
    "\n",
    "print('saved model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['id','text']\n",
    "my_dev = pd.read_csv(\"agr_en_sm_test.csv\",header=None, names=cols)\n",
    "#my_dev.drop(['id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = []\n",
    "Zs = []\n",
    "cs = tokenize_data(my_dev.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in cs:\n",
    "    Zs.append(text_processor.pre_process_doc(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "count = 0\n",
    "with open('segmentation_test.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in Zs:        \n",
    "        writer.writerow([row])\n",
    "        #filtered_words = ([word for word in row])\n",
    "        #write(str(filtered_words)+\"\\n\")\n",
    "        count = count +1\n",
    "f.close()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['text']\n",
    "my_dev = pd.read_csv(\"segmentation_fb_test.csv\",header=None)\n",
    "#my_dev = pd.read_csv(\"segmentation_fb_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(my_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dev[0] = my_dev[0].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# we're still going to use a Tokenizer here, but we don't need to fit it\n",
    "tokenizer = Tokenizer(num_words=train_x.shape[1])\n",
    "# for human-friendly printing\n",
    "labels = ['OAG','CAG','NAG']\n",
    "\n",
    "# read in our saved dictionary\n",
    "with open('dictionary.json', 'r') as dictionary_file:\n",
    "    dictionary = json.load(dictionary_file)\n",
    "\n",
    "# this utility makes sure that all the words in your input\n",
    "# are registered in the dictionary\n",
    "# before trying to turn them into a matrix.\n",
    "not_found_word_list = []\n",
    "def convert_text_to_index_array(text):\n",
    "    words = kpt.text_to_word_sequence(text,filters='',lower=False)\n",
    "    wordIndices = []\n",
    "    no_word = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            wordIndices.append(dictionary[word])\n",
    "        else:\n",
    "            #print(\"'%s' not in training corpus; ignoring.\" %(word))\n",
    "            not_found_word_list.append(word)\n",
    "            no_word = no_word + 1\n",
    "    return wordIndices,no_word\n",
    "\n",
    "# read in your saved model structure\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "# and create a model from that\n",
    "model = model_from_json(loaded_model_json)\n",
    "# and weight your nodes with your saved values\n",
    "model.load_weights('model.h5')\n",
    "with open('fileName.csv', 'w') as f:\n",
    "    count=0\n",
    "    no_words = 0\n",
    "    for row in my_dev[0]:\n",
    "        # okay here's the interactive part\n",
    "        evalSentence = row\n",
    "        # format your input for the neural net\n",
    "        testArr,no_word = convert_text_to_index_array(evalSentence)\n",
    "        input = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "        # predict which bucket your input belongs in\n",
    "        pred = model.predict(input)\n",
    "        # and print it for the humons\n",
    "        f.write(labels[np.argmax(pred)] + \"\\n\")\n",
    "        #f.write(pred + \"\\n\")\n",
    "        count+=1\n",
    "        no_words+=no_word\n",
    "f.close()\n",
    "print(count)\n",
    "print(\"word not found : \", no_words)\n",
    "with open('not_found_word_list.csv', 'w') as f:\n",
    "    for word in not_found_word_list:\n",
    "        f.write(str(word)+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
